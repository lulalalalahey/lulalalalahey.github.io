# 总结
实习时间一共是八周（其实是九周，加上培训）。 工作内容跟我想的不太一样，但是还是挺满意的。我的工作主要是由两部分组成：
## 1. copilot agent
这部分内容主要是使用copilot studio内置的功能完成一些助手。<br>
在此期间我主要是完成了两种agent，尝试了一种agent(帮助计算公司账目的，失败了最后)。三种比较成功的是
1. 帮助公司制作bio。
就是这家公司需要每年制作大量的bio，这种琐碎的工作之前都是由员工自己完成，耗费了大量的时间，因此我的supervisor希望借助AI提高他们的工作效率。我们的任务就是处理用户发送的bio并且转化成符合公司要求的格式。<br><br>  我的思路如下：因为AI助手对格式化据的理解能力更强，因此我要求AI助手先把用户发送的信息拆解成json格式的信息，然后利用这些json格式的信息汇总成符合公司要求的bio。
<br>
为了实现这个目的，我制作了两个表格，一个是教给AI如何拆解数据。我规定input是raw bio，然后我利用文档内置的copilot，要求它把这个raw bio拆解成json格式的数据，这些数据包括了姓名，职称，过往经历，现在的职位和责任以及学历等。另外一个文档的输入和输出和上一个文档相反（知识现在的output是公司风格的bio）。我之所以分为两个文档是为了让AI更好的理解如何拆解raw文档，然后利用得到的json格式的数据如何制作公司风格的bio。然后在instruction部分再加入我的细则的要求，比如不要写当事人的年龄，第一段要求full name，后面才可以加职称，或者是大使必须要写ambassador XXX之类的。给用户返回的回答是先罗列提取到的重要信息，然后就是修改之后的公司风格的bio。<br>
我认为我自己的这个处理方式虽然比较麻烦，但也是我目前想到的最优解。我的同事没有对数据进行处理，他所用的就是sharepoint中的raw data也就是word文档、pdf之类的表现非常不好（但是他觉得特别好，哈哈）。但我这种方法的弊端也很明显，虽然准确性相对高，但是处理速度比较慢。因此我还在想是否有更简单的方法。

2. 第二个是制作newsclips的agent.<br>
公司需要持续跟进membership最新的动向，因此要求实习生每天制作newsclips，帮助员工更好的了解公司的membership（我猜可能是为了保持员工的参与度，让员工了解公司在做什么，它的合作伙伴在做什么，更好的指定商业方案maybe）。但这个工作非常耗费时间，往往员工需要登录特定的GMail，这个邮箱设定了alert，每天上午某个特定时段开始推送相关公司的新闻，员工需要自己选择十个左右的link（高度相关的，而且一般都是财经类的），然后把内容复制粘贴到一个word中制作newsclips，他们还需要自己在网上找两个不是membership的信息。因此我的任务是制作自动化agent。<br><br> 这项工作非常折磨人（过程我就不说了，非常难受）。我把这个任务分成了两个步骤，第一个是向员工推送rss形式的email，第二步是员工向agent发送link，然后总结成newsclips。<br>为什么我在有GMail的前提下，还是制作了推送flow，理由是：Google和Microsoft是两家公司😥。我没办法直接用GMail。那有人可能会想，为啥不能员工挑选完link，直接让我这个flow帮他总结成newsclips不完了，那另外一个问题就是公司不想为API支付额外的费用，所以一开始我没尝试用API，而是直接抓取（但是不久我的IP被ban了，疑似新闻网站把我当成恶意爬虫了😭），后来就是用了一个免费的API（ScraperAPI），这个的弊端就是有的domain用不了，所以我只能通过第一步限制我的domain（白名单），用户用第一步的rss新闻才能使用第二步的newsclips maker。但是这还是非常不方便的。第一，rss返回的不是direct link，用户需要先打开了之后复制真实的link，才能使用第二步。第二，这个这个还是不能避免第二步报错，因为有的时候http操作老抽风，非常不稳定。第三，API抓取每个月是有限制的。第四，耗费时间很久。第五，这个还是没办法筛选低价值的新闻。<br>
但这个是我目前能做到最好的了。清洗数据的时候借助的AI，其他时候基本上就是纯写代码，但是这个flow只能是图形化的（类似于乐高的编程），非常不方便，我问gpt它有时候还老给我旧版本或者是根本不存在的操作，非常头痛。
3. 提取旧文件的有价值信息。
